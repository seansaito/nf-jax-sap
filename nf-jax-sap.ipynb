{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Real-NVP to generate the SAP logo from a Gaussian distribution\n",
    "\n",
    "This notebook uses [Real-NVP](https://arxiv.org/abs/1605.08803), a type of normalizing flow, in order to generate a complex distribution (SAP logo) from a simple one (Gaussian).\n",
    "\n",
    "It is implemented using [JAX](https://jax.readthedocs.io/) and most of the code is borrowed from Eric Jang's [implementation](https://github.com/ericjang/nf-jax) of normalizing flows.\n",
    "\n",
    "Here is the gist of changes done to the original implementation to make Real-NVP work for the SAP logo (as opposed to a more simple distribution like the half-moons used in Eric Jang's tutorial):\n",
    "\n",
    "* The hidden layers in each normalizing flow are wider (128 => 1024 units) and deeper (> 2 stacks of Dense + Activation)\n",
    "* Activation layer changed from ReLU to LeakyReLU\n",
    "* Regularization (L2 weight decay) and gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct target image as a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import jax.numpy as np\n",
    "from jax import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Initialize parameters, not committing to a batch shape\n",
    "rng = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mpimg.imread('sap-logo.jpg')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img[350,401])\n",
    "print(img[100,600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_img_to_scatter(img, stride, hit):\n",
    "    # Convert an image into a scatter plot\n",
    "    X = []\n",
    "    for i in range(0, img.shape[0] - 50, stride):\n",
    "        for j in range(0, img.shape[1], stride):\n",
    "            if list(img[img.shape[0]-(i+1),j]) == hit:\n",
    "                \n",
    "                \n",
    "                X.append([j, i])\n",
    "    return np.array(X)\n",
    "\n",
    "def sample_n01(N, D=2):\n",
    "    return random.normal(rng, (N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = convert_img_to_scatter(img, 2, [25, 119, 209])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = sample_n01(X.shape[0])\n",
    "X_noisy = X + noise\n",
    "plt.scatter(X_noisy[:, 0], X_noisy[:, 1], s=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_n01(x, eps=1e-15):\n",
    "    return np.sum(-np.square(x)/2 - np.log(np.sqrt(2*np.pi)),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(log_prob_n01(X_noisy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_noisy = StandardScaler().fit_transform(X_noisy)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "plt.hist(log_prob_n01(X_noisy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(log_prob_n01(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_noisy[:, 0], X_noisy[:, 1], s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-NVP implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import stax # neural network library\n",
    "from jax.experimental.stax import Dense, Relu, LeakyRelu\n",
    "\n",
    "def nvp_forward(net_params, shift_and_log_scale_fn, x, flip=False):\n",
    "    d = x.shape[-1] // 2\n",
    "    x1, x2 = x[:, :d], x[:, d:]\n",
    "    if flip:\n",
    "        x2, x1 = x1, x2\n",
    "        \n",
    "    shift, log_scale = shift_and_log_scale_fn(net_params, x1)\n",
    "    y2 = x2 * np.exp(log_scale) + shift\n",
    "    if flip:\n",
    "        x1, y2 = y2, x1\n",
    "    y = np.concatenate([x1, y2], axis=-1)\n",
    "    return y\n",
    "\n",
    "def nvp_inverse(net_params, shift_and_log_scale_fn, y, flip=False):\n",
    "    d = y.shape[-1] // 2\n",
    "    y1, y2 = y[:, :d], y[:, d:]\n",
    "    if flip:\n",
    "        y1, y2 = y2, y1\n",
    "    \n",
    "    shift, log_scale = shift_and_log_scale_fn(net_params, y1)\n",
    "    x2 = (y2 - shift) * np.exp(-log_scale)\n",
    "    if flip:\n",
    "        y1, x2 = x2, y1\n",
    "    x = np.concatenate([y1, x2], axis=-1)\n",
    "    return x, log_scale\n",
    "\n",
    "def init_nvp():\n",
    "    \"\"\"\n",
    "    Number of layers and hidden units can have a \n",
    "    significant effect on performance and final output.\n",
    "    \"\"\"\n",
    "    D = 2\n",
    "    net_init, net_apply = stax.serial(\n",
    "       Dense(2048), LeakyRelu, Dense(2048), LeakyRelu, Dense(2048), LeakyRelu, \n",
    "       Dense(2048), LeakyRelu, Dense(2048), LeakyRelu, Dense(2048), LeakyRelu, \n",
    "       Dense(2048), LeakyRelu, Dense(2048), LeakyRelu, Dense(2048), LeakyRelu, \n",
    "       Dense(2048), LeakyRelu, Dense(2048), LeakyRelu, Dense(2048), LeakyRelu, \n",
    "       Dense(2048), LeakyRelu, Dense(2048), LeakyRelu, Dense(2048), LeakyRelu, \n",
    "       Dense(D)\n",
    "    )\n",
    "    in_shape = (-1, D//2)\n",
    "    out_shape, net_params = net_init(rng, in_shape)\n",
    "    \n",
    "    def shift_and_log_scale_fn(net_params, x1):\n",
    "        s = net_apply(net_params, x1)\n",
    "        return np.split(s, 2, axis=1)\n",
    "    \n",
    "    return net_params, shift_and_log_scale_fn\n",
    "\n",
    "def sample_nvp(net_params, shift_and_log_scale_fn, base_sample_fn, N, flip=False):\n",
    "    x = base_sample_fn(N)\n",
    "    return nvp_forward(net_params, shift_and_log_scale_fn, x, flip)\n",
    "\n",
    "def log_prob_nvp(net_params, shift_and_log_scale_fn, base_log_prob_fn, y, flip=False):\n",
    "    x, log_scale = nvp_inverse(net_params, shift_and_log_scale_fn, y, flip)\n",
    "    ildj = -np.sum(log_scale, axis=-1)\n",
    "    return base_log_prob_fn(x) + ildj\n",
    "\n",
    "def init_nvp_chain(n=2):\n",
    "    flip = False\n",
    "    ps, configs = [], []\n",
    "    for i in range(n):\n",
    "        p, f = init_nvp()\n",
    "        ps.append(p), configs.append((f, flip))\n",
    "        flip = not flip\n",
    "    return ps, configs\n",
    "\n",
    "def sample_nvp_chain(ps, configs, base_sample_fn, N):\n",
    "    x = base_sample_fn(N)\n",
    "    for p, config in zip(ps, configs):\n",
    "        shift_and_log_scale_fn, flip = config\n",
    "        x = nvp_forward(p, shift_and_log_scale_fn, x, flip)\n",
    "    return x\n",
    "\n",
    "def make_log_prob_fn(p, log_prob_fn, config):\n",
    "    shift_and_log_scale_fn, flip = config\n",
    "    return lambda x: log_prob_nvp(p, shift_and_log_scale_fn, log_prob_fn, x, flip)\n",
    "\n",
    "def log_prob_nvp_chain(ps, configs, base_log_prob_fn, y):\n",
    "    log_prob_fn = base_log_prob_fn\n",
    "    for p, config in zip(ps, configs):\n",
    "        log_prob_fn = make_log_prob_fn(p, log_prob_fn, config)\n",
    "    return log_prob_fn(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import optimizers\n",
    "from jax import jit, grad\n",
    "import numpy as onp\n",
    "from tqdm import tqdm\n",
    "\n",
    "hp = {\n",
    "    'chains': 15,\n",
    "    'beta': 1e-3,\n",
    "    'grad_clip': 1.0,\n",
    "    'epochs': 5e5,\n",
    "    'eta': 1e-5,\n",
    "    'hidden': 2048\n",
    "}\n",
    "\n",
    "ps, cs = init_nvp_chain(hp['chains'])\n",
    "\n",
    "def loss(params, batch):\n",
    "    return -np.mean(log_prob_nvp_chain(\n",
    "        params, cs, log_prob_n01, batch)) + hp['beta'] * optimizers.l2_norm(params)\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size=hp['eta'])\n",
    "\n",
    "@jit\n",
    "def step(i, opt_state, batch):\n",
    "    params = get_params(opt_state)\n",
    "    g = grad(loss)(params, batch)\n",
    "    g = optimizers.clip_grads(g, hp['grad_clip'])\n",
    "    l = loss(params, batch)\n",
    "    return opt_update(i, g, opt_state), l\n",
    "    \n",
    "iters = int(hp['epochs'])\n",
    "data_generator = (X_noisy[onp.random.choice(X_noisy.shape[0], 100)] for _ in range(iters))\n",
    "opt_state = opt_init(ps)\n",
    "losses = []\n",
    "for i in tqdm(range(iters)):\n",
    "    opt_state, l = step(i, opt_state, next(data_generator))\n",
    "    losses.append(np.log(l))\n",
    "\n",
    "ps = get_params(opt_state)\n",
    "\n",
    "plt.plot(losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sample_nvp_chain(ps, cs, sample_n01, 3000)\n",
    "print(y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y[:, 0], y[:, 1], s=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animate as GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML, Image\n",
    "\n",
    "x = sample_n01(3000)\n",
    "values = [x]\n",
    "for p, config in zip(ps, cs):\n",
    "    shift_log_scale_fn, flip = config\n",
    "    x = nvp_forward(p, shift_log_scale_fn, x, flip=flip)\n",
    "    values.append(x)\n",
    "    \n",
    "# First set up the figure, the axis, and the plot element we want to animate\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "values = [values[0]] * 5 + values + [values[-1]] * 5\n",
    "\n",
    "y = values[0]\n",
    "paths = ax.scatter(y[:, 0], y[:, 1], s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 12\n",
    "\n",
    "def animate(i):\n",
    "    l = i//n\n",
    "    t = (float(i%n))/n\n",
    "    y = (1-t)*values[l] + t*values[l+1]\n",
    "    paths.set_offsets(y)\n",
    "    return (paths,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = 'sap_logo_nf_{chains}_{beta}_{eta}_{grad_clip}_{epochs}_{hidden}.gif'.format(**hp)\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames=n*(len(cs) + 5 + 5), interval=1, blit=False)\n",
    "anim.save(f_name, writer='imagemagick', fps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url=f_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
